{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utf-8\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(\"https://www.thsrc.com.tw/tw/TimeTable/SearchResult\")\n",
    "print(response.encoding)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<option value=\"2f940836-cedc-41ef-8e28-c2336ac8fe68\"> 南港站</option>,\n",
       " <option value=\"977abb69-413a-4ccf-a109-0272c24fd490\"> 台北站</option>,\n",
       " <option value=\"e6e26e66-7dc1-458f-b2f3-71ce65fdc95f\"> 板橋站</option>,\n",
       " <option value=\"fbd828d8-b1da-4b06-a3bd-680cdca4d2cd\"> 桃園站</option>,\n",
       " <option value=\"a7a04c89-900b-4798-95a3-c01c455622f4\"> 新竹站</option>,\n",
       " <option value=\"e8fc2123-2aaf-46ff-ad79-51d4002a1ef3\"> 苗栗站</option>,\n",
       " <option value=\"3301e395-46b8-47aa-aa37-139e15708779\"> 台中站</option>,\n",
       " <option value=\"38b8c40b-aef0-4d66-b257-da96ec51620e\"> 彰化站</option>,\n",
       " <option value=\"5f4c7bb0-c676-4e39-8d3c-f12fc188ee5f\"> 雲林站</option>,\n",
       " <option value=\"60831846-f0e4-47f6-9b5b-46323ebdcef7\"> 嘉義站</option>,\n",
       " <option value=\"9c5ac6ca-ec89-48f8-aab0-41b738cb1814\"> 台南站</option>,\n",
       " <option value=\"f2519629-5973-4d08-913b-479cce78a356\"> 左營站</option>,\n",
       " <option value=\"2f940836-cedc-41ef-8e28-c2336ac8fe68\"> 南港站</option>,\n",
       " <option value=\"977abb69-413a-4ccf-a109-0272c24fd490\"> 台北站</option>,\n",
       " <option value=\"e6e26e66-7dc1-458f-b2f3-71ce65fdc95f\"> 板橋站</option>,\n",
       " <option value=\"fbd828d8-b1da-4b06-a3bd-680cdca4d2cd\"> 桃園站</option>,\n",
       " <option value=\"a7a04c89-900b-4798-95a3-c01c455622f4\"> 新竹站</option>,\n",
       " <option value=\"e8fc2123-2aaf-46ff-ad79-51d4002a1ef3\"> 苗栗站</option>,\n",
       " <option value=\"3301e395-46b8-47aa-aa37-139e15708779\"> 台中站</option>,\n",
       " <option value=\"38b8c40b-aef0-4d66-b257-da96ec51620e\"> 彰化站</option>,\n",
       " <option value=\"5f4c7bb0-c676-4e39-8d3c-f12fc188ee5f\"> 雲林站</option>,\n",
       " <option value=\"60831846-f0e4-47f6-9b5b-46323ebdcef7\"> 嘉義站</option>,\n",
       " <option value=\"9c5ac6ca-ec89-48f8-aab0-41b738cb1814\"> 台南站</option>,\n",
       " <option value=\"f2519629-5973-4d08-913b-479cce78a356\"> 左營站</option>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(\"option\", {\"value\":re.compile(\"[a-z0-9]{8}-[a-z0-9]{4}\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(soup.find(\"section\", class_ = \"result_table\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "form_data = {\"StartStation\":\"2f940836-cedc-41ef-8e28-c2336ac8fe68\",\n",
    "             \"EndStation\":\"e6e26e66-7dc1-458f-b2f3-71ce65fdc95f\",\n",
    "             \"SearchDate\":\"2017/08/13\",\n",
    "             \"SearchTime\":\"20:30\",\n",
    "             \"SearchWay\":\"DepartureInMandarin\"}\n",
    "response_post = requests.post(\"https://www.thsrc.com.tw/tw/TimeTable/SearchResult\", data = form_data)\n",
    "soup_post = BeautifulSoup(response_post.text, \"lxml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup_post.find(\"section\", class_ = \"result_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# your codes\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 將要查詢的資料寫成 dictionary\n",
    "form_data = {\"StartStation\":\"2f940836-cedc-41ef-8e28-c2336ac8fe68\",\n",
    "             \"EndStation\":\"9c5ac6ca-ec89-48f8-aab0-41b738cb1814\",\n",
    "             \"SearchDate\":\"2017/08/14\",\n",
    "             \"SearchTime\":\"21:30\",\n",
    "             \"SearchWay\":\"DepartureInMandarin\"}\n",
    "\n",
    "# requests 改用 POST，並放入剛剛寫好的 dictionary\n",
    "response_post = requests.post(\"https://www.thsrc.com.tw/tw/TimeTable/SearchResult\", data = form_data)\n",
    "soup_post = BeautifulSoup(response_post.text, \"lxml\")\n",
    "\n",
    "# 觀察高鐵網頁後，發現車次資訊藏在 屬性 class=column1 的標籤裡面\n",
    "print(len(soup_post.find_all(\"td\",class_=\"column1\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>Python 爬蟲實戰</h1>\n"
     ]
    }
   ],
   "source": [
    "# import 套件\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 用 requests 抓取網頁 https://jimmy15923.github.io/example_page 並存在 response\n",
    "response = requests.get(\"https://jimmy15923.github.io/example_page\")\n",
    "\n",
    "# 用 BS4 解析 HTML 並把結果回傳 soup (lxml 是 BeautifulSoup 的解析器，預設是使用 html.parser，但是 lxml 的速度及性能較佳)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "# 印出 h1 標籤\n",
    "print(soup.find(\"h1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>Python 爬蟲實戰</h1>\n",
      "<h1>Python 爬蟲實戰</h1>\n",
      "<h1>Python 爬蟲實戰</h1>\n",
      "<h1>Python 爬蟲實戰</h1>\n",
      "<h1>Python 爬蟲實戰</h1>\n"
     ]
    }
   ],
   "source": [
    "print(soup.find(\"h1\"))\n",
    "print(soup.h1)\n",
    "print(soup.html.h1)\n",
    "print(soup.body.h1)\n",
    "print(soup.html.body.h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "response = requests.get(\"https://jimmy15923.github.io/example_page\")\n",
    "soup = BeautifulSoup(response.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<td> 列2 欄1 </td>\n",
      "\n",
      "\n",
      " 列2 欄1 \n"
     ]
    }
   ],
   "source": [
    "print(soup.find(\"td\"))  # 找出第一個名為 td 的標籤\n",
    "print(\"\\n\") # 換行符號，讓兩個 Print 的結果中間可以隔一個空行\n",
    "print(soup.find(\"td\").text)     # 找出第一個名為 td 的標籤並印出其文字內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.ResultSet'>\n",
      "\n",
      "\n",
      "[<td> 列2 欄1 </td>, <td class=\"zzz\"> 列2 欄2 (我的屬性 class=\"zzz\") </td>, <td>\n",
      "<a href=\"http://www.yahoo.com.tw\">列2 欄3 (我是 a 標籤，屬性 href=網址) </a>\n",
      "</td>, <td>\n",
      "<a href=\"http://foundation.datasci.tw/\">列2 欄4 (資料協會) </a>\n",
      "</td>, <td value=\"5566\">列3 欄1 </td>, <td>列3 欄2\n",
      "\t\t\t\t<ol>\n",
      "<li class=\"zz\">我是 li 標籤 (列表)，屬性 class=\"zz\" </li>\n",
      "<li> 第二個 li 標籤 </li>\n",
      "</ol>\n",
      "</td>, <td>\n",
      "<a href=\"http://foundation.datasci.tw/python-crawling-170813/\" id=\"hyperlink\"> 列3 欄3 (資料協會-python 爬蟲實戰)</a>\n",
      "</td>, <td class=\"zzzz\">列3 欄4 (我的屬性 class=\"zzzz\")</td>]\n"
     ]
    }
   ],
   "source": [
    "print(type(soup.find_all(\"td\"))) # find_all 回傳的是 list\n",
    "print(\"\\n\")\n",
    "print(soup.find_all(\"td\")) # 找出所有 td 的標籤，並回傳 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"zzz\" id=\"id1\">我是有著屬性 class=\"zzz\" 的標籤內容</div>, <td class=\"zzz\"> 列2 欄2 (我的屬性 class=\"zzz\") </td>]\n",
      "\n",
      "\n",
      "[<div class=\"zzz\" id=\"id1\">我是有著屬性 class=\"zzz\" 的標籤內容</div>, <td class=\"zzz\"> 列2 欄2 (我的屬性 class=\"zzz\") </td>]\n"
     ]
    }
   ],
   "source": [
    "print(soup.find_all(\"\", {\"class\":\"zzz\"}))     # 不指定標籤，但找出所有屬性 class = \"zzz\" 的標籤\n",
    "print(\"\\n\")\n",
    "print(soup.find_all(\"\", class_=\"zzz\"))     # 不同寫法 但有一樣的結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python_crawler', 'python_crawler']\n"
     ]
    }
   ],
   "source": [
    "print(soup.find_all(text = \"python_crawler\"))   # 找出所有標籤文字內容等於 python_crawler 的次數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 範例網頁: \"https://jimmy15923.github.io/example_page\"\n",
    "# 1. import 套件\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 2. 發送 requests.get，並將結果存在 response (或自己定義喜歡的變數也可以)\n",
    "# your codes\n",
    "response = requests.get(\"https://jimmy15923.github.io/example_page\")\n",
    "\n",
    "# 3. 將 response 的 HTML 文字放進 BeautifulSoup，並將結果存在 soup (或自己定義喜歡的變數也可以)\n",
    "# your codes\n",
    "soup = BeautifulSoup(response.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<td> 列2 欄1 </td>, <td class=\"zzz\"> 列2 欄2 (我的屬性 class=\"zzz\") </td>, <td>\n",
      "<a href=\"http://www.yahoo.com.tw\">列2 欄3 (我是 a 標籤，屬性 href=網址) </a>\n",
      "</td>, <td>\n",
      "<a href=\"http://foundation.datasci.tw/\">列2 欄4 (資料協會) </a>\n",
      "</td>, <td value=\"5566\">列3 欄1 </td>, <td>列3 欄2\n",
      "\t\t\t\t<ol>\n",
      "<li class=\"zz\">我是 li 標籤 (列表)，屬性 class=\"zz\" </li>\n",
      "<li> 第二個 li 標籤 </li>\n",
      "</ol>\n",
      "</td>, <td>\n",
      "<a href=\"http://foundation.datasci.tw/python-crawling-170813/\" id=\"hyperlink\"> 列3 欄3 (資料協會-python 爬蟲實戰)</a>\n",
      "</td>, <td class=\"zzzz\">列3 欄4 (我的屬性 class=\"zzzz\")</td>]\n"
     ]
    }
   ],
   "source": [
    "print((soup.find_all(\"td\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是有著屬性 class=\"zzz\" 的標籤內容\n"
     ]
    }
   ],
   "source": [
    "print(soup.find(\"div\", id = \"id1\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://foundation.datasci.tw/python-crawling-170813/\n"
     ]
    }
   ],
   "source": [
    "print(soup.find(\"a\",id=\"hyperlink\")[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abbbbc', 'bc']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "### your codes\n",
    "pattern = \"a*b+c\"\n",
    "test_string = 'find abbbbc, bc, skip c, acc'\n",
    "re.findall(pattern, test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abbbc', 'bc']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = 'a*b+c'\n",
    "test_string = 'find abbbc, bc, skip c, acc'\n",
    "re.findall(pattern, test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12', '11', '10']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = \"[0-9]+\"\n",
    "test_string = '12 drummers drumming, 11 pipers piping, 10 lords a-leaping'\n",
    "re.findall(pattern, test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "email_text = \"\"\"\n",
    "Big Data Analytics/ Deep LearningSocial Computing / Computational Social Science / Crowdsourcing\n",
    "Multimediaand Network SystemsQuality of ExperienceInformation SecurityPh.D. candidate at NTU EEchihfan02-27883799#1602Camera CalibrationComputer VisionData\n",
    "Analysiscmchang02-27883799#1671System OptimizationMachine LearningyusraBig data\n",
    "analysiscclin02-27883799#1668Data Analysisrusi02-27883799#1668Government Procurement ActFinancial\n",
    "Managementkatekuen02-27883799#1602AdministrationEvent Planningseanyu02-27883799#1668Data \n",
    "AnalysisPsychology & NeuroscienceMarketingxinchinchenEmbedded Systemkyoyachuan062602-27883799\n",
    "#1601FinTechActuarial ScienceData Analysiskai0604602-27883799#1601Data AnalysisMachine Learningchloe02-27839427Accountingafun02-27883799 afun@iis.sinica.edu.tw\n",
    "#1673Data AnalysisWeb developmentyunhsu198902-27883799#1668MarketingTIGP Ph.D. Fellow at Academia Sinica & NCCUbaowalyMachine LearningData AnalysisSocial Computingchangyc1427883799#1678\n",
    "Data Analysisjimmy1592302-2788379 jimmy15923@iis.sinica.com.tw#1688Data AnalysisjasontangAnalysisMachine Learninguchen02-27883799#1668Deep Learningpjwu02-27883799#1604Computational PhotographyData Analysis\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jimmy15923@iis.sinica.com.tw', 'com')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "re.findall(\"([A-Za-z0-9._]+@[A-Za-z.]+(com|edu)\\.tw)\", email_text)\n",
    "re.findall(\"([A-Za-z0-9._]+@[A-Za-z.]+(com)\\.tw)\", email_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTF-8\n",
      "['02-29242789', '03-4709933', '04-23601719', '06-2092929', '05-2238686', '07-6994433', '07-3610768', '02-29662939', '02-29662939', '02-29609370']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# your codes\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "response = requests.get(\"http://yp.518.com.tw/service-life.html?ctf=10\") # requests 518 網頁 並拿到 response\n",
    "print(response.encoding) # 印出 encoding 結果\n",
    "soup = BeautifulSoup(response.text, \"lxml\")  # 將 HTML 丟給 BeautifulSoup 作解析\n",
    "\n",
    "# 找到電話資訊都藏在 li 標籤，屬性 class=comp_tel\n",
    "all_phone_text = [tag.text for tag in soup.find_all(\"li\",class_=\"comp_tel\")]\n",
    "\n",
    "# 把電話資訊的 list 存成一個大 string\n",
    "all_phone_text =\"\".join(all_phone_text)\n",
    "\n",
    "# 用 regular expression 把全部的電話都找出來\n",
    "phone_number = re.findall(\"0[1-9]+-[0-9]+\", all_phone_text)\n",
    "print(phone_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTF-8\n",
      "['02-29242789', '03-4709933', '04-23601719', '06-2092929', '05-2238686', '07-6994433', '07-3610768', '02-29662939', '02-29662939', '02-29609370']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "respone = requests.get(\"http://yp.518.com.tw/service-life.html?ctf=10\")\n",
    "print(response.encoding)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "all_phone_text = [tag.text for tag in soup.find_all(\"li\", {\"class\":\"comp_tel\"})]\n",
    "all_phone_text = \"\".join(all_phone_text)\n",
    "\n",
    "phone_number = re.findall(\"0[1-9]+-[0-9]+\", all_phone_text)\n",
    "print(phone_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
